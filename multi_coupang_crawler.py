import sys
import re
import time
import json
import random
import argparse
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count

import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from fake_useragent import UserAgent

# ---------------------------------------------------------
# 1. Helper Functions (Data Cleaning)
# ---------------------------------------------------------

def get_product_code(url: str) -> str:
    """URLì—ì„œ ìƒí’ˆ ì½”ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        if "products/" in url:
            return url.split("products/")[-1].split("?")[0]
    except:
        pass
    return "unknown"

def get_num_in_str(element: str) -> int:
    """ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        return int(re.sub(r'[^0-9]', '', element))
    except:
        return 0

def get_star_rating(element: str) -> float: 
    """style ì†ì„±(width: %)ì—ì„œ ë³„ì ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        rating_percent = float(re.sub(r'[^0-9]', '', element))
        # 100% = 5ì  -> 20ìœ¼ë¡œ ë‚˜ëˆ”
        avg_rating = round((rating_percent / 20), 2) 
        return avg_rating
    except:
        return 0.0

def replace_thumbnail_size(url: str) -> str:
    """ì¸ë„¤ì¼ ì´ë¯¸ì§€ URLì„ ë” í° ì‚¬ì´ì¦ˆë¡œ ë³€ê²½í•©ë‹ˆë‹¤."""
    if not url:
        return ""
    return re.sub(r'/remote/[^/]+/image', '/remote/292x292ex/image', url)

# ---------------------------------------------------------
# 2. Driver Setup (Optimized)
# ---------------------------------------------------------

def setup_optimized_driver(proxy_ip: str = None, proxy_port: int = None) -> uc.Chrome:
    """
    Sets up the undetected_chromedriver.
    NOTE: Coupang heavily detects headless browsers. 
    Using standard GUI mode (similar to simple_coupang_crawler.py) for stability.
    """
    options = uc.ChromeOptions()
    # options.add_argument("--headless=new")  # ìµœì‹  í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš©
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--disable-infobars")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-popup-blocking")
    options.add_argument("--start-maximized")
    options.add_argument("--incognito")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    
    # Add proxy if provided
    if proxy_ip and proxy_port:
        options.add_argument(f'--proxy-server={proxy_ip}:{proxy_port}')
    
    # Random User Agent
    random_ua = UserAgent().random
    options.add_argument(f'--user-agent={random_ua}')
    
    # macOS í™˜ê²½ ë° ë²„ì „ í˜¸í™˜ì„± ê³ ë ¤ (simple_coupang_crawler.py ì°¸ê³ )
    driver = None
    for attempt in range(3):
        try:
            driver = uc.Chrome(
                options=options, 
                enable_cdp_events=True, 
                incognito=True, 
                version_main=142  # ì„¤ì¹˜ëœ Chrome ë²„ì „ì— ë§ì¶° ì„¤ì •
            )
            break
        except FileNotFoundError:
            # ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œ íŒŒì¼ ê²½í•©ìœ¼ë¡œ ë°œìƒ ê°€ëŠ¥
            time.sleep(random.uniform(1, 2))
        except Exception as e:
            if attempt == 2:
                raise e
            time.sleep(random.uniform(1, 2))
            
    return driver

# ---------------------------------------------------------
# 3. Core Crawling Logic (Single Process)
# ---------------------------------------------------------

def crawl_single_product(url: str, proxy_ip: str = None, proxy_port: int = None) -> dict:
    """
    ë‹¨ì¼ URLì— ëŒ€í•´ ë…ë¦½ì ì¸ ë“œë¼ì´ë²„ë¥¼ ë„ìš°ê³  ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    result = {
        "url": url,
        "status": "failed",
        "data": {},
        "error": None
    }
    
    driver = None
    try:
        driver = setup_optimized_driver(proxy_ip, proxy_port)
        
        # í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        driver.set_page_load_timeout(30)
        driver.get(url)
        
        # ìŠ¤ë§ˆíŠ¸ ëŒ€ê¸°: ìƒí’ˆ ì œëª©ì´ ëœ° ë•Œê¹Œì§€ ìµœëŒ€ 20ì´ˆ ëŒ€ê¸°
        wait = WebDriverWait(driver, 20)
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'h1.product-title')))
        except TimeoutException:
            # ì œëª©ì´ ì•ˆ ëœ¨ë©´ ë¡œë”© ì‹¤íŒ¨ë¡œ ê°„ì£¼
            raise Exception("Page load timeout or bot detection")

        # ë°ì´í„° ì¶”ì¶œì„ ìœ„í•œ Dictionary
        product_dict = {}
        product_dict['product_code'] = get_product_code(url)

        # 1. Title
        try:
            title = driver.find_element(By.CSS_SELECTOR, 'h1.product-title').text
            product_dict['title'] = title
        except:
            product_dict['title'] = "N/A"

        # 2. Image
        try:
            # ì´ë¯¸ì§€ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ì–´ë„ src ì†ì„±ì€ ìˆì„ ìˆ˜ ìˆìŒ
            image_url = driver.find_element(By.CSS_SELECTOR, 'div.product-image img').get_attribute('src')
            product_dict['image_url'] = replace_thumbnail_size(image_url)
        except:
            product_dict['image_url'] = ""

        # 3. Categories
        try:
            categorys = driver.find_elements(By.CSS_SELECTOR, 'ul.breadcrumb li')
            category_list = [c.text for c in categorys[1:]] # ì²« ë²ˆì§¸ëŠ” ë³´í†µ í™ˆì´ë¯€ë¡œ ì œì™¸
            product_dict['categories'] = category_list
        except:
            product_dict['categories'] = []

        # 4. Rating
        try:
            el = driver.find_element(By.CSS_SELECTOR, 'span.rating-star-num').get_attribute("style")
            product_dict['star_rating'] = get_star_rating(el)
        except:
            product_dict['star_rating'] = 0.0

        # 5. Review Count
        try:
            el = driver.find_element(By.CSS_SELECTOR, 'span.rating-count-txt').text
            product_dict['review_count'] = get_num_in_str(el)
        except:
            product_dict['review_count'] = 0

        # 6. Prices
        try:
            # ì •ê°€
            sales_price_el = driver.find_elements(By.CSS_SELECTOR, 'div.price-amount.sales-price-amount')
            product_dict['original_price'] = get_num_in_str(sales_price_el[0].text) if sales_price_el else 0
            
            # íŒë§¤ê°€
            final_price_el = driver.find_elements(By.CSS_SELECTOR, 'div.price-amount.final-price-amount')
            product_dict['final_price'] = get_num_in_str(final_price_el[0].text) if final_price_el else 0
        except:
            product_dict['original_price'] = 0
            product_dict['final_price'] = 0

        result["status"] = "success"
        result["data"] = product_dict

    except Exception as e:
        result["error"] = str(e)
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass
    
    return result

# ---------------------------------------------------------
# 4. Parallel Execution Controller
# ---------------------------------------------------------

def run_parallel_crawling(urls: list, max_workers: int = None, proxy_ip: str = None, proxy_port: int = None):
    """
    ì£¼ì–´ì§„ URL ë¦¬ìŠ¤íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    if max_workers is None:
        # CPU ì½”ì–´ì˜ 80% ì •ë„ë§Œ ì‚¬ìš© (ë„ˆë¬´ ë§ì´ ë„ìš°ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡±/ì°¨ë‹¨ ìœ„í—˜)
        max_workers = max(1, int(cpu_count() * 0.8))
    
    print(f"[INFO] Starting parallel crawling with {max_workers} workers for {len(urls)} URLs...")
    
    results = []
    start_time = time.time()

    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # Future ê°ì²´ ìƒì„±
        future_to_url = {executor.submit(crawl_single_product, url, proxy_ip, proxy_port): url for url in urls}
        
        # ì™„ë£Œë˜ëŠ” ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ì²˜ë¦¬
        for i, future in enumerate(as_completed(future_to_url)):
            url = future_to_url[future]
            try:
                res = future.result()
                results.append(res)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                status_icon = "âœ…" if res['status'] == "success" else "âŒ"
                print(f"[{i+1}/{len(urls)}] {status_icon} Processed: {url[-30:]}...")
                
            except Exception as exc:
                print(f"[{i+1}/{len(urls)}] ğŸ’¥ System Error for {url}: {exc}")

    end_time = time.time()
    duration = end_time - start_time
    
    print("\n" + "="*60)
    print(f" [CRAWLING SUMMARY]")
    print(f" - Total URLs: {len(urls)}")
    print(f" - Success: {len([r for r in results if r['status'] == 'success'])}")
    print(f" - Failed: {len([r for r in results if r['status'] == 'failed'])}")
    print(f" - Total Time: {duration:.2f} seconds")
    print("="*60 + "\n")
    
    return results

# ---------------------------------------------------------
# 5. Main Entry Point
# ---------------------------------------------------------

if __name__ == "__main__":
    # Windows í™˜ê²½ì—ì„œ multiprocessing ì‚¬ìš© ì‹œ í•„ìˆ˜
    from multiprocessing import freeze_support
    freeze_support()

    parser = argparse.ArgumentParser(description="Multi-Process Coupang Crawler")
    parser.add_argument("urls", nargs="*", help="List of Coupang product URLs separated by space")
    parser.add_argument("--file", "-f", help="File path containing URLs (one per line)", type=str)
    parser.add_argument("--workers", "-w", help="Number of parallel workers", type=int, default=None)
    parser.add_argument("--proxy-ip", help="Proxy server IP address", type=str, default=None)
    parser.add_argument("--proxy-port", help="Proxy server port", type=int, default=None)
    
    args = parser.parse_args()
    
    target_urls = []
    
    # 1. ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ URLì´ ë“¤ì–´ì˜¨ ê²½ìš°
    if args.urls:
        target_urls.extend(args.urls)
        
    # 2. íŒŒì¼ë¡œ ì£¼ì–´ì§„ ê²½ìš°
    if args.file:
        try:
            with open(args.file, 'r', encoding='utf-8') as f:
                file_urls = [line.strip() for line in f if line.strip()]
                target_urls.extend(file_urls)
        except Exception as e:
            print(f"[ERROR] Failed to read file: {e}")

    # 3. ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ ì…ë ¥ ë°›ê¸°
    if not target_urls:
        print("Enter Coupang product URLs (comma separated):")
        raw_input = input("URLs: ").strip()
        if raw_input:
            target_urls = [u.strip() for u in raw_input.split(',')]

    if target_urls:
        # [ì¤‘ìš”] ë³‘ë ¬ ì‹¤í–‰ ì „ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë“œë¼ì´ë²„ë¥¼ í•œ ë²ˆ ì´ˆê¸°í™”í•˜ì—¬ 
        # ë°”ì´ë„ˆë¦¬ íŒ¨ì¹˜ ë° ë‹¤ìš´ë¡œë“œ ê²½ìŸ ìƒíƒœ(Race Condition)ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        print("[INFO] Pre-initializing chromedriver to prevent race conditions...")
        try:
            dummy_driver = setup_optimized_driver(args.proxy_ip, args.proxy_port)
            dummy_driver.quit()
            print("[INFO] Driver initialized successfully.")
        except Exception as e:
            print(f"[WARN] Driver pre-initialization failed (will retry in workers): {e}")

        # ì‹¤í–‰
        final_results = run_parallel_crawling(target_urls, max_workers=args.workers, proxy_ip=args.proxy_ip, proxy_port=args.proxy_port)
        
        # ê²°ê³¼ JSON ì¶œë ¥
        print(json.dumps(final_results, indent=4, ensure_ascii=False))
    else:
        print("[ERROR] No URLs provided.")
